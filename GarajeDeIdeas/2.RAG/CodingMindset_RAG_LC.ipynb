{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CodingMindset - RAG App - LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai langchain-chroma pypdf\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "\n",
    "documents = PyPDFDirectoryLoader(\"./GarajeDeIdeas/2.RAG/data/\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_chain.invoke({\"input\": \"What is mamba?\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is mamba?',\n",
       " 'context': [Document(page_content='genomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model\\nbackbone.\\nAcknowledgments\\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.\\nReferences\\n[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. “Unitary Evolution Recurrent Neural Networks”. In: The\\nInternational Conference on Machine Learning (ICML) . 2016, pp. 1120–1128.\\n[2] iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor,\\nYannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. “Eﬀective Gene Expression Prediction from\\nSequence by Integrating Long-range Interactions”. In: Nature Methods 18.10 (2021), pp. 1196–1203.\\n[3] Jimmy Ba, Geoﬀrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. “Using Fast Weights to\\nAttend to the Recent Past”. In: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016).', metadata={'page': 17, 'source': 'GarajeDeIdeas/2.RAG/data/mamba.pdf'}),\n",
       "  Document(page_content='genomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model\\nbackbone.\\nAcknowledgments\\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.\\nReferences\\n[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. “Unitary Evolution Recurrent Neural Networks”. In: The\\nInternational Conference on Machine Learning (ICML) . 2016, pp. 1120–1128.\\n[2] iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor,\\nYannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. “Eﬀective Gene Expression Prediction from\\nSequence by Integrating Long-range Interactions”. In: Nature Methods 18.10 (2021), pp. 1196–1203.\\n[3] Jimmy Ba, Geoﬀrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. “Using Fast Weights to\\nAttend to the Recent Past”. In: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016).', metadata={'page': 17, 'source': 'GarajeDeIdeas/2.RAG/data/mamba.pdf'}),\n",
       "  Document(page_content='key to large language models, Mamba not only solves them easily but can extrapolate solutions indeﬁnitely long\\n(>1M tokens).\\n•Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform-\\ners on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g.\\nreducing FID on a challenging speech generation dataset by more than half). In both settings, its performance\\nimproves with longer context up to million-length sequences.\\n•Language Modeling. Mamba is the ﬁrst linear-time sequence model that truly achieves Transformer-quality\\nperformance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters,\\nwe show that Mamba exceeds the performance of a large range of baselines, including very strong modern\\nTransformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5 ×', metadata={'page': 1, 'source': 'GarajeDeIdeas/2.RAG/data/mamba.pdf'}),\n",
       "  Document(page_content='key to large language models, Mamba not only solves them easily but can extrapolate solutions indeﬁnitely long\\n(>1M tokens).\\n•Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform-\\ners on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g.\\nreducing FID on a challenging speech generation dataset by more than half). In both settings, its performance\\nimproves with longer context up to million-length sequences.\\n•Language Modeling. Mamba is the ﬁrst linear-time sequence model that truly achieves Transformer-quality\\nperformance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters,\\nwe show that Mamba exceeds the performance of a large range of baselines, including very strong modern\\nTransformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5 ×', metadata={'page': 1, 'source': 'GarajeDeIdeas/2.RAG/data/mamba.pdf'})],\n",
       " 'answer': 'Mamba is a sequence model backbone that has shown strong performance in various tasks like genomics, audio, and video. It outperforms previous state-of-the-art models in tasks such as modeling audio waveforms and DNA sequences. Additionally, Mamba is the first linear-time sequence model to achieve Transformer-quality performance.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creamos una función de ayuda, una vez recuperamos los documentos, los juntamos para añadirlos como contexto al prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "lcel_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mamba is a strong candidate for a general sequence model backbone that out-performs prior state-of-the-art models in audio and genomics tasks, showing superior performance with longer context up to million-length sequences. It is the first linear-time sequence model to achieve Transformer-quality performance in language modeling, exceeding the performance of other baselines even with 1B parameters.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcel_chain.invoke(\"What is mamba?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
