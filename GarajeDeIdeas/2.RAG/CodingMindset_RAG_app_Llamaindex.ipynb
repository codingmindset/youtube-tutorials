{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CodingMindset - RAG App - LLamaIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORUGv5MSIKnx",
        "outputId": "2558f9ec-c36f-434b-b1e8-0fca5d40f5da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index openai pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4FM043C_CU97"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import getpass\n",
        "\n",
        "openai.api_key = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5s5Jhw8JBGx"
      },
      "source": [
        "## Naive RAG App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BBtDZEwMH15j"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./GarajeDeIdeas/2.RAG/data/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sDNMULNMw2W",
        "outputId": "d7c2eee5-daae-4789-8fbd-9e2c6279a1a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'llama_index.core.schema.Document'>\n",
            "Doc ID: af63bb7f-a2df-433c-8aa0-47f8a1f39d31\n",
            "Text: Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
            "Albert Gu *1and Tri Dao *2 1Machine Learning Department, Carnegie\n",
            "Mellon University 2Department of Computer Science, Princeton\n",
            "University agu@cs.cmu.edu ,tri@tridao.me Abstract Foundation models,\n",
            "now powering most of the exciting applications in deep learning, are\n",
            "almost universall...\n"
          ]
        }
      ],
      "source": [
        "print(type(documents[0]))\n",
        "print(documents[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6p3YqCVyNmcc"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import VectorStoreIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dzqdD1hNOCyV"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "rC-MoTIxLixV"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_chAQ3VOgPF",
        "outputId": "2402bcba-b564-4ca3-e104-a6283f077e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mamba is a model that excels in various domains such as audio and genomics, language modeling, and scaling laws for sequence models. It outperforms previous state-of-the-art models like SaShiMi, Hyena, and Transformers in tasks such as modeling audio waveforms, DNA sequences, and language processing. Mamba demonstrates superior performance in pretraining quality and downstream metrics, showing improvements with longer context lengths. Additionally, Mamba is the first linear-time sequence model to achieve Transformer-quality performance and has open-sourced model code and pre-trained checkpoints available on GitHub.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"What is mamba?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsft2HcpLFJc"
      },
      "source": [
        "### Customizing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KU2j0hKLO4cO"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uZ-aBzA7MUtE"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cah-fwAyMbXE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mamba is a model that excels in various domains such as audio waveforms, DNA sequences, and language modeling. It outperforms previous state-of-the-art models like SaShiMi, Hyena, and Transformers in terms of pretraining quality and downstream metrics. Mamba demonstrates superior performance with longer context lengths and has been shown to achieve Transformer-quality performance in language modeling tasks. The model has open-sourced code and pre-trained checkpoints available on GitHub.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"What is mamba?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aesbmorNj3G"
      },
      "source": [
        "### Persisting the index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "G4opHhMPMpkp"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "index.storage_context.persist()\n",
        "\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "index = load_index_from_storage(storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GYyGj-sKN2ZA"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mamba is a model that excels in various domains such as audio waveforms, DNA sequences, and language modeling. It outperforms previous state-of-the-art models in pretraining quality and downstream metrics. In the field of language modeling, Mamba is the first linear-time sequence model to achieve Transformer-quality performance, surpassing other baselines in terms of performance and efficiency.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"What is mamba?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
